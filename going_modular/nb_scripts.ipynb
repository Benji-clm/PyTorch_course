{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e798207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dest_path = Path(\"going_modular\")\n",
    "dest_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f81acbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/data_setup.py\n",
    "\"\"\"\n",
    "Contains functionality for creating PyTorch DataLoaders for \n",
    "image classification data.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# NUM_WORKERS = os.cpu_count()\n",
    "NUM_WORKERS = 0 # set to zero for windows devices (change for Linux)\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "  \"\"\"Creates training and testing DataLoaders.\n",
    "\n",
    "  Takes in a training directory and testing directory path and turns\n",
    "  them into PyTorch Datasets and then into PyTorch DataLoaders.\n",
    "\n",
    "  Args:\n",
    "    train_dir: Path to training directory.\n",
    "    test_dir: Path to testing directory.\n",
    "    transform: torchvision transforms to perform on training and testing data.\n",
    "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
    "    num_workers: An integer for number of workers per DataLoader.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of (train_dataloader, test_dataloader, class_names).\n",
    "    Where class_names is a list of the target classes.\n",
    "    Example usage:\n",
    "      train_dataloader, test_dataloader, class_names = \\\n",
    "        = create_dataloaders(train_dir=path/to/train_dir,\n",
    "                             test_dir=path/to/test_dir,\n",
    "                             transform=some_transform,\n",
    "                             batch_size=32,\n",
    "                             num_workers=4)\n",
    "  \"\"\"\n",
    "  # Use ImageFolder to create dataset(s)\n",
    "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "  # Get class names\n",
    "  class_names = train_data.classes\n",
    "\n",
    "  # Turn images into data loaders\n",
    "  train_dataloader = DataLoader(\n",
    "      train_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "  test_dataloader = DataLoader(\n",
    "      test_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False, # don't need to shuffle test data\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "\n",
    "  return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "987ead08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/model_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/model_builder.py\n",
    "\"\"\"\n",
    "Contains PyTorch model code to instantiate a custom ResNet model.\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample=False):\n",
    "        super().__init__()\n",
    "        stride = 2 if downsample else 1\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.downsample = nn.Sequential()\n",
    "        if downsample or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.downsample(x)\n",
    "        out = self.conv_block(x)\n",
    "        return self.relu(out + identity)\n",
    "\n",
    "\n",
    "class ResNetCustom(nn.Module):\n",
    "    def __init__(self, input_channels: int, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.layer1 = ResidualBlock(64, 64)\n",
    "        self.layer2 = ResidualBlock(64, 128, downsample=True)\n",
    "        self.layer3 = ResidualBlock(128, 256, downsample=True)\n",
    "        self.layer4 = ResidualBlock(256, 512, downsample=True)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d23812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/engine.py\n",
    "\"\"\"\n",
    "Contains functions for training and testing a PyTorch model.\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "  \"\"\"Trains a PyTorch model for a single epoch.\n",
    "\n",
    "  Turns a target PyTorch model to training mode and then\n",
    "  runs through all of the required training steps (forward\n",
    "  pass, loss calculation, optimizer step).\n",
    "\n",
    "  Args:\n",
    "    model: A PyTorch model to be trained.\n",
    "    dataloader: A DataLoader instance for the model to be trained on.\n",
    "    loss_fn: A PyTorch loss function to minimize.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "  Returns:\n",
    "    A tuple of training loss and training accuracy metrics.\n",
    "    In the form (train_loss, train_accuracy). For example:\n",
    "\n",
    "    (0.1112, 0.8743)\n",
    "  \"\"\"\n",
    "  # Put model in train mode\n",
    "  model.train()\n",
    "\n",
    "  # Setup train loss and train accuracy values\n",
    "  train_loss, train_acc = 0, 0\n",
    "\n",
    "  # Loop through data loader data batches\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "      # Send data to target device\n",
    "      X, y = X.to(device), y.to(device)\n",
    "\n",
    "      # 1. Forward pass\n",
    "      y_pred = model(X)\n",
    "\n",
    "      # 2. Calculate  and accumulate loss\n",
    "      loss = loss_fn(y_pred, y)\n",
    "      train_loss += loss.item() \n",
    "\n",
    "      # 3. Optimizer zero grad\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # 4. Loss backward\n",
    "      loss.backward()\n",
    "\n",
    "      # 5. Optimizer step\n",
    "      optimizer.step()\n",
    "\n",
    "      # Calculate and accumulate accuracy metric across all batches\n",
    "      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "  # Adjust metrics to get average loss and accuracy per batch \n",
    "  train_loss = train_loss / len(dataloader)\n",
    "  train_acc = train_acc / len(dataloader)\n",
    "  return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "  \"\"\"Tests a PyTorch model for a single epoch.\n",
    "\n",
    "  Turns a target PyTorch model to \"eval\" mode and then performs\n",
    "  a forward pass on a testing dataset.\n",
    "\n",
    "  Args:\n",
    "    model: A PyTorch model to be tested.\n",
    "    dataloader: A DataLoader instance for the model to be tested on.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "  Returns:\n",
    "    A tuple of testing loss and testing accuracy metrics.\n",
    "    In the form (test_loss, test_accuracy). For example:\n",
    "\n",
    "    (0.0223, 0.8985)\n",
    "  \"\"\"\n",
    "  # Put model in eval mode\n",
    "  model.eval() \n",
    "\n",
    "  # Setup test loss and test accuracy values\n",
    "  test_loss, test_acc = 0, 0\n",
    "\n",
    "  # Turn on inference context manager\n",
    "  with torch.inference_mode():\n",
    "      # Loop through DataLoader batches\n",
    "      for batch, (X, y) in enumerate(dataloader):\n",
    "          # Send data to target device\n",
    "          X, y = X.to(device), y.to(device)\n",
    "\n",
    "          # 1. Forward pass\n",
    "          test_pred_logits = model(X)\n",
    "\n",
    "          # 2. Calculate and accumulate loss\n",
    "          loss = loss_fn(test_pred_logits, y)\n",
    "          test_loss += loss.item()\n",
    "\n",
    "          # Calculate and accumulate accuracy\n",
    "          test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "  # Adjust metrics to get average loss and accuracy per batch \n",
    "  test_loss = test_loss / len(dataloader)\n",
    "  test_acc = test_acc / len(dataloader)\n",
    "  return test_loss, test_acc\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device) -> Dict[str, List]:\n",
    "  \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "  Passes a target PyTorch models through train_step() and test_step()\n",
    "  functions for a number of epochs, training and testing the model\n",
    "  in the same epoch loop.\n",
    "\n",
    "  Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "  Args:\n",
    "    model: A PyTorch model to be trained and tested.\n",
    "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "    epochs: An integer indicating how many epochs to train for.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "  Returns:\n",
    "    A dictionary of training and testing loss as well as training and\n",
    "    testing accuracy metrics. Each metric has a value in a list for \n",
    "    each epoch.\n",
    "    In the form: {train_loss: [...],\n",
    "                  train_acc: [...],\n",
    "                  test_loss: [...],\n",
    "                  test_acc: [...]} \n",
    "    For example if training for epochs=2: \n",
    "                 {train_loss: [2.0616, 1.0537],\n",
    "                  train_acc: [0.3945, 0.3945],\n",
    "                  test_loss: [1.2641, 1.5706],\n",
    "                  test_acc: [0.3400, 0.2973]} \n",
    "  \"\"\"\n",
    "  # Create empty results dictionary\n",
    "  results = {\"train_loss\": [],\n",
    "      \"train_acc\": [],\n",
    "      \"test_loss\": [],\n",
    "      \"test_acc\": []\n",
    "  }\n",
    "\n",
    "  # Loop through training and testing steps for a number of epochs\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "      train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer=optimizer,\n",
    "                                          device=device)\n",
    "      test_loss, test_acc = test_step(model=model,\n",
    "          dataloader=test_dataloader,\n",
    "          loss_fn=loss_fn,\n",
    "          device=device)\n",
    "\n",
    "      # Print out what's happening\n",
    "      print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "      )\n",
    "\n",
    "      # Update results dictionary\n",
    "      results[\"train_loss\"].append(train_loss)\n",
    "      results[\"train_acc\"].append(train_acc)\n",
    "      results[\"test_loss\"].append(test_loss)\n",
    "      results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "  # Return the filled results at the end of the epochs\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9c04b95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/utils.py\n",
    "\"\"\"\n",
    "Contains various utility functions for PyTorch model training and saving.\n",
    "\"\"\"\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def get_food101_subset(destination: str = \"data/pizza_steak_sushi\", classes: list = None) -> Path:\n",
    "    \"\"\"\n",
    "    Prepares a subset of Food-101 from already-extracted data.\n",
    "    Expects the full dataset to be in: data/food-101/images/\n",
    "    \"\"\"\n",
    "    base_path = Path(__file__).parent.parent.parent.parent / \"data\"\n",
    "    dest_path = Path(destination)\n",
    "\n",
    "    if not base_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Expected Food-101 images at {base_path}. Please extract the full dataset there.\"\n",
    "        )\n",
    "\n",
    "    if dest_path.exists():\n",
    "        print(f\"{dest_path} already exists. Skipping subset creation.\")\n",
    "        return dest_path\n",
    "\n",
    "    if classes is None:\n",
    "        classes = [\"pizza\", \"steak\", \"sushi\"]\n",
    "\n",
    "    import shutil\n",
    "    import random\n",
    "    dest_path.mkdir(parents=True, exist_ok=True)\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        for cls in classes:\n",
    "            src = base_path / \"food-101\" / \"images\" / cls\n",
    "            dst = dest_path / split / cls\n",
    "            dst.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            all_images = list(src.glob(\"*.jpg\"))\n",
    "            random.shuffle(all_images)\n",
    "            n_total = 1000 if split == \"train\" else 250\n",
    "            for img in all_images[:n_total]:\n",
    "                shutil.copy(img, dst)\n",
    "\n",
    "    print(f\"Subset created at {dest_path}\")\n",
    "    return dest_path\n",
    "\n",
    "def save_model(model: torch.nn.Module,\n",
    "               model_name: str):\n",
    "  \"\"\"Saves a PyTorch model to a target directory.\n",
    "\n",
    "  Args:\n",
    "    model: A target PyTorch model to save.\n",
    "    model_name: name of the saved model\n",
    "  \"\"\"\n",
    "  # Create target directory\n",
    "  MODEL_PATH = Path(__file__).parent / \"models\"\n",
    "  MODEL_PATH.mkdir(parents=True,\n",
    "                    exist_ok=True)\n",
    "  MODEL_NAME = \"Reduced_Food101_resnet\"\n",
    "  MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "  # Save the model state_dict()\n",
    "  print(f\"[INFO] Saving model to: {MODEL_SAVE_PATH}\")\n",
    "  torch.save(obj=model.state_dict(),\n",
    "             f=MODEL_SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b041626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/train.py\n",
    "\"\"\"\n",
    "Trains a PyTorch image classification model using device-agnostic code.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import data_setup, engine, model_builder, utils\n",
    "import argparse\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "# Argparse method for setting hyperparameters\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "  prog='train.py',\n",
    "  description='Train a custom ResNet model on a Food101 subset',\n",
    "  epilog='Rip my GPU'\n",
    ")\n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=20, help='Number of training epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')\n",
    "# parser.add_argument('--hidden_units', type=int, default=10, help='Number of hidden units within layers')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='Learning rate for optimizer')\n",
    "parser.add_argument('--model_name', type=str, default='models/Reduced_Food101_resnet', help='Path to save or load model')\n",
    "\n",
    "def main():\n",
    "\n",
    "  args = parser.parse_args()\n",
    "\n",
    "  # Setup hyperparameters\n",
    "  NUM_EPOCHS = args.epochs\n",
    "  BATCH_SIZE = args.batch_size\n",
    "  # HIDDEN_UNITS = args.hidden_units\n",
    "  LEARNING_RATE = args.lr\n",
    "  MODEL_NAME= args.model_name\n",
    "\n",
    "  # Setup directories\n",
    "  image_path = utils.get_food101_subset()\n",
    "  train_dir = image_path / \"train\"\n",
    "  print(f\"train_dir: {train_dir}\")\n",
    "  test_dir = image_path / \"test\"\n",
    "\n",
    "  # Setup target device\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "  # Create transforms\n",
    "  data_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "  ])\n",
    "\n",
    "  # Create DataLoaders with help from data_setup.py\n",
    "  train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "      train_dir=train_dir,\n",
    "      test_dir=test_dir,\n",
    "      transform=data_transform,\n",
    "      batch_size=BATCH_SIZE\n",
    "  )\n",
    "\n",
    "  # Create model with help from model_builder.py\n",
    "  model = model_builder.ResNetCustom(\n",
    "      input_channels=3,\n",
    "      num_classes=len(class_names)\n",
    "  ).to(device)\n",
    "\n",
    "  # Set loss and optimizer\n",
    "  loss_fn = torch.nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(),\n",
    "                              lr=LEARNING_RATE)\n",
    "\n",
    "  # Start training with help from engine.py\n",
    "  engine.train(model=model,\n",
    "              train_dataloader=train_dataloader,\n",
    "              test_dataloader=test_dataloader,\n",
    "              loss_fn=loss_fn,\n",
    "              optimizer=optimizer,\n",
    "              epochs=NUM_EPOCHS,\n",
    "              device=device)\n",
    "\n",
    "  # Save the model with help from utils.py\n",
    "  utils.save_model(model=model,\n",
    "                  model_name=MODEL_NAME)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f0576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523a7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
